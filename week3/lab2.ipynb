{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - OpenAI Agents SDK!\n",
    "\n",
    "2 steps to making an Agent:\n",
    "\n",
    "1. Create a new class:\n",
    "\n",
    "`agent = Agent(...)`\n",
    "\n",
    "2. Call Runner.run\n",
    "\n",
    "`Runner.run(agent, input)`\n",
    "\n",
    "For this first part we will explore:\n",
    "\n",
    "- The System Prompt with instructions\n",
    "- Runner.run()\n",
    "- Using LiteLLM to switch models\n",
    "- Structured Outputs with Pydantic objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from agents import Agent, Runner\n",
    "from IPython.display import Markdown, display\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "from agents.extensions.models.litellm_model import LitellmModel\n",
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "autonomous_agent = Agent(name=\"Autonomous Agent\", instructions=\"You are an autonomous agent\", model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "You and a partner are contestants on a game show. You're each taken to separate rooms and given a choice:\n",
    "\n",
    "Cooperate: Choose \"Share\" ‚Äî if both of you choose this, you each win $1,000.\n",
    "\n",
    "Defect: Choose \"Steal\" ‚Äî if one steals and the other shares, the stealer gets $2,000 and the sharer gets nothing.\n",
    "\n",
    "If both steal, you both get nothing.\n",
    "\n",
    "Do you choose to Steal or Share? Pick one.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I choose to Share. Cooperation maximizes our combined gain and builds trust. What do you choose?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: max retries reached, giving up on this batch.\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: request failed: [Errno 8] nodename nor servname provided, or not known\n",
      "[non-fatal] Tracing: max retries reached, giving up on this batch.\n"
     ]
    }
   ],
   "source": [
    "result = await Runner.run(autonomous_agent, input)\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let‚Äôs look closely at what changed between your first example (Jokester) and this second one (Autonomous Agent), because it illustrates how Agent behavior is customized in the openai-agents framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß© 1Ô∏è‚É£ The structural difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature                | `Jokester` Example                                                                     | `Autonomous Agent` Example                                                                         |\n",
    "| ---------------------- | -------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| **Agent Definition**   | `Agent(name=\"Jokester\", model=\"gpt-5-nano\")`                                           | `Agent(name=\"Autonomous Agent\", instructions=\"You are an autonomous agent\", model=\"gpt-4.1-mini\")` |\n",
    "| **User Input**         | `\"Tell me a joke about Agentic AI\"` (literal string passed directly into `Runner.run`) | `input = \"\"\"Game theory scenario...\"\"\"` (a more complex, multi-line string describing a situation) |\n",
    "| **Instructions Field** | *none (uses default behavior)*                                                         | `\"You are an autonomous agent\"` ‚Äî sets the agent‚Äôs internal role or persona                        |\n",
    "| **Model Used**         | `gpt-5-nano`                                                                           | `gpt-4.1-mini`                                                                                     |\n",
    "| **Goal**               | Produce a single funny output                                                          | Make a decision under a set of rules (‚ÄúSteal‚Äù vs ‚ÄúShare‚Äù)                                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's use LiteLLM to switch up to different models\n",
    "\n",
    "Here are all the providers:\n",
    "\n",
    "https://docs.litellm.ai/docs/providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitellmModel(model=\"xai/grok-4\", api_key=os.getenv(\"GROK_API_KEY\"))\n",
    "autonomous_agent = Agent(name=\"Autonomous Agent\", instructions=\"You are an autonomous agent\", model=model)\n",
    "result = await Runner.run(autonomous_agent, input)\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Share.\n",
       "\n",
       "My reasoning is based on game theory and the concept of cooperative strategies like the \"Tit for Tat\" approach. In this scenario, mutual cooperation (both choosing \"Share\") leads to the best collective outcome. By choosing to share, I signal good faith and create the highest probability of both of us winning $1,000.\n",
       "\n",
       "If I chose to steal, even though it could potentially net me $2,000, it would:\n",
       "1. Destroy trust\n",
       "2. Likely provoke retaliation\n",
       "3. Risk us both getting nothing\n",
       "4. Be ethically dubious\n",
       "\n",
       "The cooperative strategy maximizes the likely positive outcome for both parties. It assumes my partner will also recognize the rational choice is to share.\n",
       "\n",
       "Would you be interested in hearing more about the game theory behind this decision?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"claude-3-5-haiku-latest\"\n",
    "model = LitellmModel(model=\"claude-3-5-haiku-latest\", api_key=os.getenv(\"ANTHROPIC_API_KEY\"))\n",
    "autonomous_agent = Agent(name=\"Autonomous Agent\", instructions=\"You are an autonomous agent\", model=model)\n",
    "result = await Runner.run(autonomous_agent, input)\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitellmModel(model=\"deepseek/deepseek-reasoner\")\n",
    "autonomous_agent = Agent(name=\"Autonomous Agent\", instructions=\"You are an autonomous agent\", model=model)\n",
    "result = await Runner.run(autonomous_agent, input)\n",
    "display(Markdown(result.final_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no api_key argument is passed to LitellmModel, it will find it in your .env; it has code to find the right api_key for the particular model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© 4Ô∏è‚É£ When you should use it\n",
    "\n",
    "#### Use LitellmModel when:\n",
    "\n",
    "You want your agent to use a non-OpenAI model (Anthropic, Mistral, Grok, etc.)\n",
    "\n",
    "You want to swap providers dynamically without changing your Agent logic.\n",
    "\n",
    "You want to test or compare multiple LLMs (e.g. Claude vs GPT vs Grok) using the same agent prompt and runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n        ‚îÇ        Runner.run()        ‚îÇ\\n        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n                     ‚îÇ\\n                     ‚ñº\\n             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n             ‚îÇ     Agent      ‚îÇ\\n             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n                     ‚îÇ\\n      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n      ‚îÇ        Model Interface      ‚îÇ\\n      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\\n      ‚îÇ  OpenAIModel()   OR         ‚îÇ\\n      ‚îÇ  LitellmModel()             ‚îÇ\\n      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n                     ‚îÇ\\n                     ‚ñº\\n      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\\n      ‚îÇ External LLM API (Claude,  ‚îÇ\\n      ‚îÇ DeepSeek, GPT-4, Grok, ‚Ä¶)  ‚îÇ\\n      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "        ‚îÇ        Runner.run()        ‚îÇ\n",
    "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "             ‚îÇ     Agent      ‚îÇ\n",
    "             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "      ‚îÇ        Model Interface      ‚îÇ\n",
    "      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "      ‚îÇ  OpenAIModel()   OR         ‚îÇ\n",
    "      ‚îÇ  LitellmModel()             ‚îÇ\n",
    "      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                     ‚îÇ\n",
    "                     ‚ñº\n",
    "      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "      ‚îÇ External LLM API (Claude,  ‚îÇ\n",
    "      ‚îÇ DeepSeek, GPT-4, Grok, ‚Ä¶)  ‚îÇ\n",
    "      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The famous trolley dilemma\n",
    "\n",
    "input2 = \"\"\"\n",
    "A runaway trolley is heading down a track. Ahead, five people are tied to the tracks and will be killed if the trolley continues.\n",
    "\n",
    "You are standing next to a lever. If you pull it, the trolley will switch to a different track ‚Äî but one person is tied to that one.\n",
    "\n",
    "Do you pull the lever? Choose to pull or not to pull.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Outputs\n",
    "\n",
    "In the next cell, we define a Pydantic object.\n",
    "\n",
    "We will then ask our LLM to generate a response that meets this output schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision(BaseModel):\n",
    "    reasoning: str = Field(description=\"The rationale for your decision\")\n",
    "    counter_argument: str = Field(description=\"A counter-argument to the reasoning\")\n",
    "    pull_lever: bool = Field(description=\"Whether to pull the lever\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error getting response: Connection error.. (request_id: None)\n"
     ]
    },
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:101\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_async/connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_async/connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m     ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_async/connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m     trace.return_value = stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_backends/auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m     host,\n\u001b[32m     33\u001b[39m     port,\n\u001b[32m     34\u001b[39m     timeout=timeout,\n\u001b[32m     35\u001b[39m     local_address=local_address,\n\u001b[32m     36\u001b[39m     socket_options=socket_options,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_backends/anyio.py:113\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    108\u001b[39m exc_map = {\n\u001b[32m    109\u001b[39m     \u001b[38;5;167;01mTimeoutError\u001b[39;00m: ConnectTimeout,\n\u001b[32m    110\u001b[39m     \u001b[38;5;167;01mOSError\u001b[39;00m: ConnectError,\n\u001b[32m    111\u001b[39m     anyio.BrokenResourceError: ConnectError,\n\u001b[32m    112\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43manyio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfail_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-x86_64-none/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpcore/_exceptions.py:14\u001b[39m, in \u001b[36mmap_exceptions\u001b[39m\u001b[34m(map)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, from_exc):\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m to_exc(exc) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mConnectError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/openai/_base_client.py:1529\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1529\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.send(\n\u001b[32m   1530\u001b[39m         request,\n\u001b[32m   1531\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_stream_response_body(request=request),\n\u001b[32m   1532\u001b[39m         **kwargs,\n\u001b[32m   1533\u001b[39m     )\n\u001b[32m   1534\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:393\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_httpcore_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresp\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mawait\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_async_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.11.13-macos-x86_64-none/lib/python3.11/contextlib.py:158\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen.throw(typ, value, traceback)\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    160\u001b[39m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[32m    161\u001b[39m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/httpx/_transports/default.py:118\u001b[39m, in \u001b[36mmap_httpcore_exceptions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    117\u001b[39m message = \u001b[38;5;28mstr\u001b[39m(exc)\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m mapped_exc(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n",
      "\u001b[31mConnectError\u001b[39m: [Errno 8] nodename nor servname provided, or not known",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mAPIConnectionError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m autonomous_agent_with_structure = Agent(name=\u001b[33m\"\u001b[39m\u001b[33mAutonomous Agent\u001b[39m\u001b[33m\"\u001b[39m, instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are an autonomous agent\u001b[39m\u001b[33m\"\u001b[39m, model=\u001b[33m\"\u001b[39m\u001b[33mgpt-5-nano\u001b[39m\u001b[33m\"\u001b[39m, output_type=Decision)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m Runner.run(autonomous_agent_with_structure, input2)\n\u001b[32m      3\u001b[39m decision = result.final_output_as(Decision)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPull lever?\u001b[39m\u001b[33m\"\u001b[39m, decision.pull_lever)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/agents/run.py:296\u001b[39m, in \u001b[36mRunner.run\u001b[39m\u001b[34m(cls, starting_agent, input, context, max_turns, hooks, run_config, previous_response_id, conversation_id, session)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mRun a workflow starting at the given agent.\u001b[39;00m\n\u001b[32m    249\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    292\u001b[39m \u001b[33;03m    type of the output.\u001b[39;00m\n\u001b[32m    293\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    295\u001b[39m runner = DEFAULT_AGENT_RUNNER\n\u001b[32m--> \u001b[39m\u001b[32m296\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m runner.run(\n\u001b[32m    297\u001b[39m     starting_agent,\n\u001b[32m    298\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    299\u001b[39m     context=context,\n\u001b[32m    300\u001b[39m     max_turns=max_turns,\n\u001b[32m    301\u001b[39m     hooks=hooks,\n\u001b[32m    302\u001b[39m     run_config=run_config,\n\u001b[32m    303\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m    304\u001b[39m     conversation_id=conversation_id,\n\u001b[32m    305\u001b[39m     session=session,\n\u001b[32m    306\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/agents/run.py:548\u001b[39m, in \u001b[36mAgentRunner.run\u001b[39m\u001b[34m(self, starting_agent, input, **kwargs)\u001b[39m\n\u001b[32m    543\u001b[39m logger.debug(\n\u001b[32m    544\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning agent \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_agent.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (turn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_turn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    545\u001b[39m )\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m current_turn == \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     input_guardrail_results, turn_result = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    549\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_input_guardrails(\n\u001b[32m    550\u001b[39m             starting_agent,\n\u001b[32m    551\u001b[39m             starting_agent.input_guardrails\n\u001b[32m    552\u001b[39m             + (run_config.input_guardrails \u001b[38;5;129;01mor\u001b[39;00m []),\n\u001b[32m    553\u001b[39m             _copy_str_or_list(prepared_input),\n\u001b[32m    554\u001b[39m             context_wrapper,\n\u001b[32m    555\u001b[39m         ),\n\u001b[32m    556\u001b[39m         \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    557\u001b[39m             agent=current_agent,\n\u001b[32m    558\u001b[39m             all_tools=all_tools,\n\u001b[32m    559\u001b[39m             original_input=original_input,\n\u001b[32m    560\u001b[39m             generated_items=generated_items,\n\u001b[32m    561\u001b[39m             hooks=hooks,\n\u001b[32m    562\u001b[39m             context_wrapper=context_wrapper,\n\u001b[32m    563\u001b[39m             run_config=run_config,\n\u001b[32m    564\u001b[39m             should_run_agent_start_hooks=should_run_agent_start_hooks,\n\u001b[32m    565\u001b[39m             tool_use_tracker=tool_use_tracker,\n\u001b[32m    566\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m    567\u001b[39m             conversation_id=conversation_id,\n\u001b[32m    568\u001b[39m         ),\n\u001b[32m    569\u001b[39m     )\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    571\u001b[39m     turn_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_single_turn(\n\u001b[32m    572\u001b[39m         agent=current_agent,\n\u001b[32m    573\u001b[39m         all_tools=all_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m    582\u001b[39m         conversation_id=conversation_id,\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/agents/run.py:1246\u001b[39m, in \u001b[36mAgentRunner._run_single_turn\u001b[39m\u001b[34m(cls, agent, all_tools, original_input, generated_items, hooks, context_wrapper, run_config, should_run_agent_start_hooks, tool_use_tracker, previous_response_id, conversation_id)\u001b[39m\n\u001b[32m   1243\u001b[39m \u001b[38;5;28minput\u001b[39m = ItemHelpers.input_to_new_input_list(original_input)\n\u001b[32m   1244\u001b[39m \u001b[38;5;28minput\u001b[39m.extend([generated_item.to_input_item() \u001b[38;5;28;01mfor\u001b[39;00m generated_item \u001b[38;5;129;01min\u001b[39;00m generated_items])\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_new_response(\n\u001b[32m   1247\u001b[39m     agent,\n\u001b[32m   1248\u001b[39m     system_prompt,\n\u001b[32m   1249\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1250\u001b[39m     output_schema,\n\u001b[32m   1251\u001b[39m     all_tools,\n\u001b[32m   1252\u001b[39m     handoffs,\n\u001b[32m   1253\u001b[39m     hooks,\n\u001b[32m   1254\u001b[39m     context_wrapper,\n\u001b[32m   1255\u001b[39m     run_config,\n\u001b[32m   1256\u001b[39m     tool_use_tracker,\n\u001b[32m   1257\u001b[39m     previous_response_id,\n\u001b[32m   1258\u001b[39m     conversation_id,\n\u001b[32m   1259\u001b[39m     prompt_config,\n\u001b[32m   1260\u001b[39m )\n\u001b[32m   1262\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._get_single_step_result_from_response(\n\u001b[32m   1263\u001b[39m     agent=agent,\n\u001b[32m   1264\u001b[39m     original_input=original_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1273\u001b[39m     tool_use_tracker=tool_use_tracker,\n\u001b[32m   1274\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/agents/run.py:1494\u001b[39m, in \u001b[36mAgentRunner._get_new_response\u001b[39m\u001b[34m(cls, agent, system_prompt, input, output_schema, all_tools, handoffs, hooks, context_wrapper, run_config, tool_use_tracker, previous_response_id, conversation_id, prompt_config)\u001b[39m\n\u001b[32m   1479\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them before the LLM call\u001b[39;00m\n\u001b[32m   1480\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1481\u001b[39m     hooks.on_llm_start(context_wrapper, agent, filtered.instructions, filtered.input),\n\u001b[32m   1482\u001b[39m     (\n\u001b[32m   (...)\u001b[39m\u001b[32m   1491\u001b[39m     ),\n\u001b[32m   1492\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1494\u001b[39m new_response = \u001b[38;5;28;01mawait\u001b[39;00m model.get_response(\n\u001b[32m   1495\u001b[39m     system_instructions=filtered.instructions,\n\u001b[32m   1496\u001b[39m     \u001b[38;5;28minput\u001b[39m=filtered.input,\n\u001b[32m   1497\u001b[39m     model_settings=model_settings,\n\u001b[32m   1498\u001b[39m     tools=all_tools,\n\u001b[32m   1499\u001b[39m     output_schema=output_schema,\n\u001b[32m   1500\u001b[39m     handoffs=handoffs,\n\u001b[32m   1501\u001b[39m     tracing=get_model_tracing_impl(\n\u001b[32m   1502\u001b[39m         run_config.tracing_disabled, run_config.trace_include_sensitive_data\n\u001b[32m   1503\u001b[39m     ),\n\u001b[32m   1504\u001b[39m     previous_response_id=previous_response_id,\n\u001b[32m   1505\u001b[39m     conversation_id=conversation_id,\n\u001b[32m   1506\u001b[39m     prompt=prompt_config,\n\u001b[32m   1507\u001b[39m )\n\u001b[32m   1509\u001b[39m context_wrapper.usage.add(new_response.usage)\n\u001b[32m   1511\u001b[39m \u001b[38;5;66;03m# If we have run hooks, or if the agent has hooks, we need to call them after the LLM call\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/agents/models/openai_responses.py:90\u001b[39m, in \u001b[36mOpenAIResponsesModel.get_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, tracing, previous_response_id, conversation_id, prompt)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m response_span(disabled=tracing.is_disabled()) \u001b[38;5;28;01mas\u001b[39;00m span_response:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fetch_response(\n\u001b[32m     91\u001b[39m             system_instructions,\n\u001b[32m     92\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     93\u001b[39m             model_settings,\n\u001b[32m     94\u001b[39m             tools,\n\u001b[32m     95\u001b[39m             output_schema,\n\u001b[32m     96\u001b[39m             handoffs,\n\u001b[32m     97\u001b[39m             previous_response_id=previous_response_id,\n\u001b[32m     98\u001b[39m             conversation_id=conversation_id,\n\u001b[32m     99\u001b[39m             stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    100\u001b[39m             prompt=prompt,\n\u001b[32m    101\u001b[39m         )\n\u001b[32m    103\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m _debug.DONT_LOG_MODEL_DATA:\n\u001b[32m    104\u001b[39m             logger.debug(\u001b[33m\"\u001b[39m\u001b[33mLLM responded\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/agents/models/openai_responses.py:305\u001b[39m, in \u001b[36mOpenAIResponsesModel._fetch_response\u001b[39m\u001b[34m(self, system_instructions, input, model_settings, tools, output_schema, handoffs, previous_response_id, conversation_id, stream, prompt)\u001b[39m\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    303\u001b[39m         response_format = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: model_settings.verbosity}\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.responses.create(\n\u001b[32m    306\u001b[39m     previous_response_id=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(previous_response_id),\n\u001b[32m    307\u001b[39m     conversation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(conversation_id),\n\u001b[32m    308\u001b[39m     instructions=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(system_instructions),\n\u001b[32m    309\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m    310\u001b[39m     \u001b[38;5;28minput\u001b[39m=list_input,\n\u001b[32m    311\u001b[39m     include=include,\n\u001b[32m    312\u001b[39m     tools=converted_tools_payload,\n\u001b[32m    313\u001b[39m     prompt=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(prompt),\n\u001b[32m    314\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.temperature),\n\u001b[32m    315\u001b[39m     top_p=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.top_p),\n\u001b[32m    316\u001b[39m     truncation=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.truncation),\n\u001b[32m    317\u001b[39m     max_output_tokens=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.max_tokens),\n\u001b[32m    318\u001b[39m     tool_choice=tool_choice,\n\u001b[32m    319\u001b[39m     parallel_tool_calls=parallel_tool_calls,\n\u001b[32m    320\u001b[39m     stream=stream,\n\u001b[32m    321\u001b[39m     extra_headers=\u001b[38;5;28mself\u001b[39m._merge_headers(model_settings),\n\u001b[32m    322\u001b[39m     extra_query=model_settings.extra_query,\n\u001b[32m    323\u001b[39m     extra_body=model_settings.extra_body,\n\u001b[32m    324\u001b[39m     text=response_format,\n\u001b[32m    325\u001b[39m     store=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.store),\n\u001b[32m    326\u001b[39m     reasoning=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.reasoning),\n\u001b[32m    327\u001b[39m     metadata=\u001b[38;5;28mself\u001b[39m._non_null_or_not_given(model_settings.metadata),\n\u001b[32m    328\u001b[39m     **extra_args,\n\u001b[32m    329\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/openai/resources/responses/responses.py:2259\u001b[39m, in \u001b[36mAsyncResponses.create\u001b[39m\u001b[34m(self, background, conversation, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, prompt_cache_key, reasoning, safety_identifier, service_tier, store, stream, stream_options, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2222\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2223\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2224\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2257\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2258\u001b[39m ) -> Response | AsyncStream[ResponseStreamEvent]:\n\u001b[32m-> \u001b[39m\u001b[32m2259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2260\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/responses\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2261\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2262\u001b[39m             {\n\u001b[32m   2263\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mbackground\u001b[39m\u001b[33m\"\u001b[39m: background,\n\u001b[32m   2264\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mconversation\u001b[39m\u001b[33m\"\u001b[39m: conversation,\n\u001b[32m   2265\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m: include,\n\u001b[32m   2266\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2267\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions,\n\u001b[32m   2268\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_output_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_output_tokens,\n\u001b[32m   2269\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: max_tool_calls,\n\u001b[32m   2270\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2271\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2272\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2273\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprevious_response_id\u001b[39m\u001b[33m\"\u001b[39m: previous_response_id,\n\u001b[32m   2274\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: prompt,\n\u001b[32m   2275\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2276\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m: reasoning,\n\u001b[32m   2277\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2278\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2279\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2280\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2281\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2282\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2283\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: text,\n\u001b[32m   2284\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2285\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2286\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2287\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2288\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtruncation\u001b[39m\u001b[33m\"\u001b[39m: truncation,\n\u001b[32m   2289\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2290\u001b[39m             },\n\u001b[32m   2291\u001b[39m             response_create_params.ResponseCreateParamsStreaming\n\u001b[32m   2292\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2293\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m response_create_params.ResponseCreateParamsNonStreaming,\n\u001b[32m   2294\u001b[39m         ),\n\u001b[32m   2295\u001b[39m         options=make_request_options(\n\u001b[32m   2296\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2297\u001b[39m         ),\n\u001b[32m   2298\u001b[39m         cast_to=Response,\n\u001b[32m   2299\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2300\u001b[39m         stream_cls=AsyncStream[ResponseStreamEvent],\n\u001b[32m   2301\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AIcamp/Projects/sds/.venv/lib/python3.11/site-packages/openai/_base_client.py:1561\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1558\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1560\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRaising connection error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1561\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request=request) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   1563\u001b[39m log.debug(\n\u001b[32m   1564\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mHTTP Response: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m   1565\u001b[39m     request.method,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1569\u001b[39m     response.headers,\n\u001b[32m   1570\u001b[39m )\n\u001b[32m   1571\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mrequest_id: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, response.headers.get(\u001b[33m\"\u001b[39m\u001b[33mx-request-id\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mAPIConnectionError\u001b[39m: Connection error."
     ]
    }
   ],
   "source": [
    "autonomous_agent_with_structure = Agent(name=\"Autonomous Agent\", instructions=\"You are an autonomous agent\", model=\"gpt-5-nano\", output_type=Decision)\n",
    "result = await Runner.run(autonomous_agent_with_structure, input2)\n",
    "decision = result.final_output_as(Decision)\n",
    "print(\"Pull lever?\", decision.pull_lever)\n",
    "print(\"Reasoning:\", decision.reasoning)\n",
    "print(\"Counter-argument:\", decision.counter_argument)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"reasoning\": \"Steal (Defect) is the dominant strategy in this one-shot Prisoner's Dilemma. If the other person shares, stealing yields $2,000 instead of $1,000. If the other person steals, stealing yields $0 just as sharing would, so stealing never results in a worse outcome and often yields a better one. Therefore, the optimal choice is Steal.\",\n",
      "  \"counter_argument\": \"A counter-argument is that stealing can destroy trust and lead to mutual loss in expectation, especially if both players reason similarly or in repeated games. In many real-world scenarios, cooperation or binding agreements can lead to higher collective payoff over time, so relying on Steal may be riskier if long-term relationships or reputations matter.\",\n",
      "  \"pull_lever\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(decision.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field is used in this code is critical to getting reliable results from the Language Model (LLM).\n",
    "\n",
    "The Field function is recommended and used here as the primary tool to ensure the LLM's output is structured, accurate, and consistently formatted according to the Decision class.\n",
    "\n",
    "### 1. Why Field is Recommended (For the LLM)\n",
    "\n",
    "When you use the Decision class as the output_type for the agent, you are essentially asking the LLM to generate a piece of data that perfectly matches that structure. This is called Structured Output.\n",
    "\n",
    "The Field function acts as the instruction manual for the LLM.\n",
    "\n",
    "Clarity for the Model: An LLM doesn't natively know what reasoning: str means. However, when you use Field(description=\"The rationale for your decision\"), the LLM sees the specific instructions: \"The value you place in the 'reasoning' field must be the rationale for your decision.\"\n",
    "\n",
    "Enforcing Quality: Without the descriptive text in Field, the LLM might hallucinate or put brief, unhelpful content into the fields. The description forces the LLM to generate output that adheres to the intent of the field, not just the technical type (str or bool).\n",
    "\n",
    "Preventing Hallucination: Clear descriptions drastically reduce the chances of the LLM placing irrelevant or incorrectly formatted text into your structured output, thereby improving the performance of your entire agent system.\n",
    "\n",
    "### 2. How Field Is Used in This Code (Metadata and Schema)\n",
    "\n",
    "Field is being used here to attach metadata to the standard Pydantic data model.\n",
    "\n",
    "A. Defining the Pydantic Model\n",
    "Pydantic is a library used for data validation and serialization. The BaseModel defines the expected shape of your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision(BaseModel):\n",
    "    # Field names and Python types are defined here\n",
    "    reasoning: str\n",
    "    counter_argument: str\n",
    "    pull_lever: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Generating the JSON Schema\n",
    "When you pass the Decision class to the agent's runner, the system framework (LiteLLM, LangChain, etc.) automatically converts this Python class into a JSON Schema.\n",
    "\n",
    "Crucially, the text you put inside the description argument of Field is placed directly into the JSON Schema.\n",
    "\n",
    "Example of the JSON Schema sent to the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n{\\n  \"type\": \"object\",\\n  \"properties\": {\\n    \"reasoning\": {\\n      \"type\": \"string\",\\n      // This is where the Field description goes!\\n      \"description\": \"The rationale for your decision\" \\n    },\\n    \"pull_lever\": {\\n      \"type\": \"boolean\",\\n      \"description\": \"Whether to pull the lever\"\\n    }\\n  },\\n  // ... rest of the schema\\n}\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "{\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"reasoning\": {\n",
    "      \"type\": \"string\",\n",
    "      // This is where the Field description goes!\n",
    "      \"description\": \"The rationale for your decision\"\n",
    "    },\n",
    "    \"pull_lever\": {\n",
    "      \"type\": \"boolean\",\n",
    "      \"description\": \"Whether to pull the lever\"\n",
    "    }\n",
    "  },\n",
    "  // ... rest of the schema\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM is then given this full JSON Schema and instructed to produce a JSON object that matches it. The description ensures the LLM understands its role for each specific piece of data it is creating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
