{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211efbd0",
   "metadata": {},
   "source": [
    "## Lab 2: the OpenAI Python Client Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8cbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fde1a4",
   "metadata": {},
   "source": [
    "### <span style=\"color: green;\">Question: what is a Client Library?</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0703645b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac48e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c59ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Tell me a fun quick fact.\"\n",
    "messages = [{\"role\": \"user\", \"content\": message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7ebb5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Did you know that honey never spoils? Archaeologists have found pots of honey in ancient Egyptian tombs that are over 3,000 years old and still perfectly edible!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    messages=messages,\n",
    "    temperature=0.5,\n",
    "    max_tokens=50)\n",
    "\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92da3d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb64df0f",
   "metadata": {},
   "source": [
    "### <span style=\"color: orange;\">Question: what is the Chat Completions API? What's the Responses API? Why use one over the other?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036b70b",
   "metadata": {},
   "source": [
    "Gemini:\n",
    "\n",
    "That's right! You don't have to manually worry about the full URL of the endpoint when using the official client library like openai = OpenAI(). ðŸ¥³\n",
    "\n",
    "The reason you don't need to specify the full URL, like \"https://api.openai.com/v1/chat/completions\", is because the client library handles that for you.\n",
    "\n",
    "How the Client Library Manages the URL\n",
    "The official client library is structured to automatically build the correct URL for every API call:\n",
    "\n",
    "1. Base URL is Hardcoded\n",
    "The main OpenAI class (OpenAI()) has the base URL hardcoded internally: https://api.openai.com/v1. This is the consistent starting point for almost all OpenAI services.\n",
    "\n",
    "2. Method Maps to Path\n",
    "The methods you call on the client object directly map to the API path:\n",
    "\n",
    "Client Method Call\tMaps to URL Path\n",
    "openai.chat.completions.create(...)\tAppends the path /chat/completions\n",
    "openai.images.generate(...)\tAppends the path /images/generations\n",
    "openai.models.list()\tAppends the path /models\n",
    "\n",
    "Export to Sheets\n",
    "When you execute a method like openai.chat.completions.create(), the client automatically combines the hardcoded base URL with the method's corresponding path to construct the full, correct endpoint URL:\n",
    "\n",
    "FullÂ EndpointÂ URL=BaseÂ URL+APIÂ Path\n",
    "FullÂ EndpointÂ URL=â€˜https://api.openai.com/v1â€˜Â +Â â€˜/chat/completionsâ€˜\n",
    "This abstraction means you focus only on what you want to do (e.g., create a chat completion), and the client library handles the low-level details of where to send the request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75613827",
   "metadata": {},
   "source": [
    "The Chat Completions API and the Responses API are the two primary ways to interact with OpenAI's large language models. The Responses API is a newer, more capable evolution of the Chat Completions API.\n",
    "\n",
    "\n",
    "## 1. Chat Completions API (/v1/chat/completions)\n",
    "What it Is: The original and industry-standard endpoint for text generation (what most LLMs use).\n",
    "\n",
    "Core Feature: It is stateless. Every request is treated independently, and you must manually manage the entire conversation history (the list of system, user, and assistant messages) and send it with every new prompt.\n",
    "\n",
    "Data Structure: Input and output are formatted as a simple array of messages.\n",
    "\n",
    "## 2. Responses API (/v1/responses)\n",
    "What it Is: The recommended, newer endpoint designed for building complex, agentic applications. It bundles the capabilities of the old Chat Completions and Assistants APIs into a single interface.\n",
    "\n",
    "Core Features:\n",
    "\n",
    "Stateful/Simplified Context: You can easily chain turns by referencing a previous_response_id or setting store: true, allowing the service to manage the conversation history for you.\n",
    "\n",
    "Agentic by Default: It natively supports complex, multi-step reasoning where the model can autonomously use tools or call functions multiple times in a single turn.\n",
    "\n",
    "Built-in Tools: It integrates powerful capabilities like web search and file search without requiring you to write custom orchestration code.\n",
    "\n",
    "Data Structure: Input and output use a more flexible concept called Items and Output to represent text, tool calls, and other complex actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f756c1",
   "metadata": {},
   "source": [
    "## 1. Chat Completions API Example (Stateless, Message-Array Input) ðŸ’¾\n",
    "The Chat Completions API is characterized by its messages array, where you must send the entire conversation history (system, user, and assistant turns) in every request to maintain context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a911dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# The client will automatically look for the OPENAI_API_KEY environment variable.\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Define the conversation history (System + User message for the first turn)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a witty, helpful assistant who loves classic literature.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the main theme of Moby Dick?\"}\n",
    "]\n",
    "\n",
    "# 2. Call the chat.completions.create endpoint\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "# 3. Extract the text\n",
    "reply = response.choices[0].message.content\n",
    "print(\"Chat Completions Reply:\", reply)\n",
    "\n",
    "# ----------------------------------------------------------------------------------\n",
    "# To continue the conversation (2nd turn):\n",
    "# You must manually append the assistant's previous reply and the new user message.\n",
    "messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "messages.append({\"role\": \"user\", \"content\": \"That's a good summary. Tell me about the author.\"})\n",
    "\n",
    "# Call the API again with the *full, updated* messages list\n",
    "response_2 = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(\"\\nSecond Turn Reply:\", response_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951256cb",
   "metadata": {},
   "source": [
    "## 2. Responses API Example (Stateful, Simplified Input) ðŸ¤–\n",
    "The Responses API simplifies single-turn requests with the input parameter and can manage state using previous_response_id for multi-turn conversations. It also makes tools easier to use.\n",
    "\n",
    "A. Simple Single-Turn Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "# 1. Simple, direct text input\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=\"You are a professional copywriter.\",\n",
    "    input=\"Create a compelling tagline for a new brand of smart coffee mugs.\"\n",
    ")\n",
    "\n",
    "# 2. Extract the text (the output structure is different)\n",
    "reply = response.output_text\n",
    "print(\"Responses API Reply:\", reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972378b1",
   "metadata": {},
   "source": [
    "B. Stateful Multi-Turn Request\n",
    "Here, the API manages the context of the joke for the follow-up question, even without passing the full message history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27245c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First turn (tell a joke)\n",
    "response_1 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"tell me a joke about Python programming.\"\n",
    ")\n",
    "print(\"Joke:\", response_1.output_text)\n",
    "\n",
    "# Second turn (follow-up question using the previous response's ID)\n",
    "# The API uses the 'id' to recall the joke and maintain context.\n",
    "response_2 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    previous_response_id=response_1.id,\n",
    "    input=\"Now, explain the technical concept that makes the joke funny.\"\n",
    ")\n",
    "print(\"\\nExplanation:\", response_2.output_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
