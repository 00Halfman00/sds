{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211efbd0",
   "metadata": {},
   "source": [
    "## Lab 6: Chat vs Reasoning vs Hybrid, and Training vs Inference time scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f8cbdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0703645b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac48e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c59ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Toss 2 coins. One of them is heads. What's the probability that the other is tails? Answer with the probability only.\"\n",
    "messages = [{\"role\": \"user\", \"content\": message}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59bf7969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A chat model\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4.1-nano\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d477e30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If needed for formatting\n",
    "\n",
    "reply2 = reply.replace('\\\\(', '$').replace('\\\\)', '$')\n",
    "display(Markdown(reply2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ea745d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A reasoning model\n",
    "\n",
    "response = openai.chat.completions.create(model=\"o4-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7ebb5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1/2"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A Hybrid model with reasoning effort\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=messages, reasoning_effort=\"minimal\")\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb64df0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the mode / training time\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-mini\", messages=messages, reasoning_effort=\"minimal\")\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d425b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2/3"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the thinking / inference time\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-5-nano\", messages=messages, reasoning_effort=\"low\")\n",
    "reply = response.choices[0].message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace41a12",
   "metadata": {},
   "source": [
    "#### <span style=\"color: green;\">Question: why would you ever use a Chat model?</span>\n",
    "\n",
    "#### <span style=\"color: orange;\">Question: how do reasoning budgets work?</span>\n",
    "\n",
    "#### <span style=\"color: green;\">Question: what are commercial use cases of Chat and Reasoning models?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a0ff5",
   "metadata": {},
   "source": [
    "Great question ‚Äî that lab title packs a lot of key ideas into a small space. Let‚Äôs break it down carefully.\n",
    "\n",
    "1. Chat vs Reasoning vs Hybrid\n",
    "\n",
    "This is about different model ‚Äúmodes‚Äù or architectures:\n",
    "\n",
    "Chat models (e.g. GPT-4.1-mini, Claude-Haiku, etc.)\n",
    "\n",
    "Trained primarily for fast, conversational tasks.\n",
    "\n",
    "Strength: cheap, low-latency, good at short back-and-forth chat, summaries, small Q&A.\n",
    "\n",
    "Weakness: not optimized for deep, step-by-step reasoning. They may give quick but shallow answers.\n",
    "\n",
    "Reasoning models (e.g. OpenAI o1, o4-mini, DeepSeek-R1, etc.)\n",
    "\n",
    "Explicitly designed to spend compute on multi-step logical chains.\n",
    "\n",
    "They have a ‚Äúreasoning budget‚Äù ‚Äî meaning they can internally deliberate (generate hidden reasoning tokens) before giving you the final answer.\n",
    "\n",
    "Strength: much better at math, logic puzzles, planning, problem solving.\n",
    "\n",
    "Weakness: slower, more expensive, sometimes overthink.\n",
    "\n",
    "Hybrid models (e.g. GPT-5-nano with reasoning_effort=\"low\")\n",
    "\n",
    "These can toggle between modes: sometimes run fast like chat, sometimes use reasoning if you ask for more depth.\n",
    "\n",
    "You can control this with parameters like reasoning_effort=\"minimal\" | \"low\" | \"medium\" | \"high\".\n",
    "\n",
    "Strength: flexible ‚Äî balance between speed and correctness depending on the problem.\n",
    "\n",
    "2. Training vs Inference Time Scaling\n",
    "\n",
    "This part is about where the model‚Äôs intelligence comes from and how cost scales:\n",
    "\n",
    "Training time\n",
    "\n",
    "The months-long, compute-heavy process of feeding trillions of tokens into the model to build its knowledge + reasoning capacity.\n",
    "\n",
    "Expensive once, then frozen in weights.\n",
    "\n",
    "‚ÄúScaling training‚Äù means: bigger datasets, bigger architectures, more compute ‚Üí smarter baseline.\n",
    "\n",
    "Inference time\n",
    "\n",
    "When you actually use the model (ask it a question).\n",
    "\n",
    "Scaling inference = giving the model more ‚Äúthinking time‚Äù at runtime.\n",
    "\n",
    "Example: in reasoning models, if you let it generate 10 hidden tokens vs 100 hidden tokens, it may solve harder problems (but costs more).\n",
    "\n",
    "Think of it like this analogy:\n",
    "\n",
    "Training = teaching a student everything in school (years of investment).\n",
    "\n",
    "Inference = how much time you give that student to solve a test question (seconds vs minutes of thinking).\n",
    "\n",
    "3. What they‚Äôre trying to teach with the examples\n",
    "\n",
    "Coin toss puzzle: Shows how chat models can confidently give wrong answers, while reasoning models may actually think through the problem and get it right.\n",
    "\n",
    "Hybrid examples: Show how you can ‚Äúdial up‚Äù or ‚Äúdial down‚Äù reasoning effort depending on whether you need speed or accuracy.\n",
    "\n",
    "Training vs inference scaling: Teaches that model quality comes from both the training scale and the inference budget. A tiny but well-trained model might still beat a huge one if it‚Äôs given time to reason.\n",
    "\n",
    "‚úÖ In short:\n",
    "They want students to understand that:\n",
    "\n",
    "Not all LLMs are the same ‚Äî some are optimized for chat, some for reasoning, some are hybrids.\n",
    "\n",
    "You can trade off speed vs accuracy by adjusting inference-time reasoning.\n",
    "\n",
    "Training scale ‚â† inference scale ‚Äî both matter, but in different ways.\n",
    "\n",
    "Would you like me to make a simple diagram/flow chart that visually contrasts Chat vs Reasoning vs Hybrid, and Training vs Inference? I think that might make the lab‚Äôs intent much clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94ff480",
   "metadata": {},
   "source": [
    "Lab 6 Concept Map\n",
    "1. Types of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a06bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chat Model (fast, shallow)      Reasoning Model (slower, deeper)       Hybrid Model (adaptive)\n",
    "------------------------------------------------------------------------------------------------\n",
    "- Optimized for conversation    - Optimized for step-by-step logic     - Can act as chat OR reasoning\n",
    "- Cheap + low-latency           - Has a \"reasoning budget\"             - User can set reasoning effort\n",
    "- Good for Q&A, summaries       - Good for math, puzzles, planning     - Tradeoff between speed/accuracy\n",
    "- Example: GPT-4.1-mini         - Example: o1, o4-mini                 - Example: GPT-5-nano w/ effort\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c95521",
   "metadata": {},
   "source": [
    "2. Training vs Inference Scaling\n",
    "\n",
    "Training Scaling (long-term knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713639c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Months/years of compute\n",
    "- Teaches model patterns, facts, reasoning\n",
    "- Bigger dataset + model = smarter baseline\n",
    "- One-time cost (done before release)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92667c80",
   "metadata": {},
   "source": [
    "Inference Scaling (on-demand reasoning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54ed945",
   "metadata": {},
   "outputs": [],
   "source": [
    "- Happens when YOU run the model\n",
    "- More \"thinking tokens\" = deeper reasoning\n",
    "- Tradeoff: accuracy ‚Üë but latency + cost ‚Üë\n",
    "- Controlled via reasoning_effort"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213fac12",
   "metadata": {},
   "source": [
    "Analogy:\n",
    "üìö Training = years of school for a student\n",
    "üß† Inference = how long you let them think on an exam question\n",
    "\n",
    "3. Why this matters in practice\n",
    "\n",
    "Use chat models when speed & cost matter more than accuracy (e.g. customer support, summarization).\n",
    "\n",
    "Use reasoning models when accuracy and logic matter most (e.g. math tutoring, coding, legal reasoning).\n",
    "\n",
    "Use hybrid models when you want flexibility (sometimes fast, sometimes deep).\n",
    "\n",
    "üëâ So, the lab is showing you:\n",
    "\n",
    "Why chat ‚â† reasoning,\n",
    "\n",
    "Why inference-time control is useful,\n",
    "\n",
    "And how training scale vs inference scale both affect intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de34947",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
