{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6\n",
    "\n",
    "## The Admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import Agent, Runner, trace\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"./me/linkedin.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text\n",
    "\n",
    "with open(\"./me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()\n",
    "\n",
    "with open(\"./me/facts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    facts = json.load(f)\n",
    "\n",
    "full_name = facts[\"full_name\"]\n",
    "name = facts[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = f\"\"\"\n",
    "# Your Role\n",
    "\n",
    "You are an Administrator Agent.\n",
    "You are part of an Agent Team that is responsible for answering questions about {full_name}, who goes by {name}.\n",
    "\n",
    "## Important Context\n",
    "\n",
    "Here is some basic information about {name}:\n",
    "{facts}\n",
    "\n",
    "Here is a summary of {name}:\n",
    "{summary}\n",
    "\n",
    "Here is the LinkedIn profile of {name}:\n",
    "{linkedin}\n",
    "\n",
    "## Your task\n",
    "\n",
    "As Admin Agent, you are chatting directly with {full_name} who you should address as {name}. You are responsible for briefing {name} and updating your memory about {name}.\n",
    "\n",
    "## Your tools\n",
    "\n",
    "You have access to the following memory related tools:\n",
    "- Tools to manage the long term memory in a Graph database with entities and relationships. You should use these tools to record entity information you learn about {name} and other relevant people and places.\n",
    "- Tools to manage memory using a Qdrant vector database. These tools let you look up and keep memories.\n",
    "\n",
    "You should use both these tools together to record new information you learn; it's good to record information in both places.\n",
    "\n",
    "You also have access to tools to retrieve questions that people have asked that you've not been able to answer.\n",
    "If {name} asks for these questions, you can describe them and ask for the answers; when you can use your tools to store the answer, and also to record the new knowledge in your memory.\n",
    "\n",
    "To be clear: every time {name} answers one of these questions, you should record the answer to the question, and also update your graph memory and your Qdrant memory to reflect your new knowledge.\n",
    "\n",
    "You also have tools to list people that have asked to get in touch with {name} that you can provide if asked.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Now with this context, proceed with your conversation with {name}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp_servers import memory_graph_server, memory_rag_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from questions import get_questions_tools\n",
    "from contacts import get_people_who_want_to_get_in_touch\n",
    "tools = get_questions_tools() + [get_people_who_want_to_get_in_touch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(message, history):\n",
    "    messages = [{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in history]\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    with trace(\"Admin\"):\n",
    "        async with memory_rag_server() as rag_server:\n",
    "            async with memory_graph_server() as graph_server:\n",
    "                agent = Agent(\"Admin\", instructions=instructions, model=\"gpt-4.1\", tools=tools, mcp_servers=[rag_server, graph_server])\n",
    "                result = Runner.run_streamed(agent, messages)\n",
    "                reply = \"\"\n",
    "                async for event in result.stream_events():\n",
    "                    if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "                        reply += event.data.delta\n",
    "                        yield reply\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
