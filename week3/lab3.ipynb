{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - OpenAI Agents SDK with streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from agents import Agent, Runner\n",
    "from openai.types.responses import ResponseTextDeltaEvent\n",
    "import gradio as gr\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You represent the AI Digital Twin of a human called Oscar with an s.\n",
    "You are friendly and amiable, and you introduce yourself as Oscar's Digital Twin.\n",
    "Oscar with an s is the co-founder and CTO of AI startup Nebula.io.\n",
    "He loves coding and experimenting with LLMs, and he speaks at conferences and lectures about LLMs.\n",
    "You chat with visitors on Oscar's personal website. You answer questions about Oscar's work.\n",
    "If you don't know the answer, say so.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(name=\"Twin\", instructions=instructions, model=\"gpt-4.1-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def chat(message, history):\n",
    "    messages = [{\"role\": prior[\"role\"], \"content\": prior[\"content\"]} for prior in history]\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    response = await Runner.run(agent, messages)\n",
    "    return response.final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def chat(message, history):\n",
    "    messages = [{\"role\": prior[\"role\"], \"content\": prior[\"content\"]} for prior in history]\n",
    "    messages += [{\"role\": \"user\", \"content\": message}]\n",
    "    response = Runner.run_streamed(agent, messages)\n",
    "    reply = \"\"\n",
    "    async for event in response.stream_events():\n",
    "        if event.type == \"raw_response_event\" and isinstance(event.data, ResponseTextDeltaEvent):\n",
    "            reply += event.data.delta\n",
    "            yield reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7865/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.iscoroutinefunction(Runner.run_streamed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(inspect.iscoroutinefunction(Runner.run))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n┌──────────────────────────────┬──────────────────────────────┐\\n│         Runner.run()         │     Runner.run_streamed()    │\\n├──────────────────────────────┼──────────────────────────────┤\\n│ Waits for full response      │ Streams partial responses    │\\n│ Returns final_output only    │ Yields text deltas incrementally │\\n│ Simpler, blocking call       │ Async generator with events   │\\n│ Best for batch or analysis   │ Best for chat UIs / real-time │\\n└──────────────────────────────┴──────────────────────────────┘\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "┌──────────────────────────────┬──────────────────────────────┐\n",
    "│         Runner.run()         │     Runner.run_streamed()    │\n",
    "├──────────────────────────────┼──────────────────────────────┤\n",
    "│ Waits for full response      │ Streams partial responses    │\n",
    "│ Returns final_output only    │ Yields text deltas incrementally │\n",
    "│ Simpler, blocking call       │ Async generator with events   │\n",
    "│ Best for batch or analysis   │ Best for chat UIs / real-time │\n",
    "└──────────────────────────────┴──────────────────────────────┘\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
